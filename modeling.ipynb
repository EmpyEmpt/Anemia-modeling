{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Окие доки, время попробовать что-то смоделировать\n",
    "# Моделирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# machine learning\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)\n",
    "plt.style.use('fivethirtyeight')\n",
    "color_pal = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "random_state = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1</th>\n",
       "      <th>A2</th>\n",
       "      <th>A3</th>\n",
       "      <th>A5</th>\n",
       "      <th>A9</th>\n",
       "      <th>A10</th>\n",
       "      <th>A11</th>\n",
       "      <th>A19</th>\n",
       "      <th>A20</th>\n",
       "      <th>A21</th>\n",
       "      <th>A23</th>\n",
       "      <th>A24</th>\n",
       "      <th>A25</th>\n",
       "      <th>A26</th>\n",
       "      <th>A28</th>\n",
       "      <th>A29</th>\n",
       "      <th>A30</th>\n",
       "      <th>A31</th>\n",
       "      <th>A32</th>\n",
       "      <th>A33</th>\n",
       "      <th>A36</th>\n",
       "      <th>B5</th>\n",
       "      <th>B7</th>\n",
       "      <th>B8</th>\n",
       "      <th>B12</th>\n",
       "      <th>B13</th>\n",
       "      <th>B14</th>\n",
       "      <th>B15</th>\n",
       "      <th>B16</th>\n",
       "      <th>B17</th>\n",
       "      <th>B18</th>\n",
       "      <th>B21</th>\n",
       "      <th>B22</th>\n",
       "      <th>B27</th>\n",
       "      <th>B34</th>\n",
       "      <th>B35</th>\n",
       "      <th>B37</th>\n",
       "      <th>B38</th>\n",
       "      <th>B39</th>\n",
       "      <th>B40</th>\n",
       "      <th>B41</th>\n",
       "      <th>B43</th>\n",
       "      <th>B44</th>\n",
       "      <th>B45</th>\n",
       "      <th>B47</th>\n",
       "      <th>B48</th>\n",
       "      <th>B49</th>\n",
       "      <th>B50</th>\n",
       "      <th>B51</th>\n",
       "      <th>B52</th>\n",
       "      <th>B55</th>\n",
       "      <th>B56</th>\n",
       "      <th>B57</th>\n",
       "      <th>B60</th>\n",
       "      <th>B61</th>\n",
       "      <th>B62</th>\n",
       "      <th>B70</th>\n",
       "      <th>B81</th>\n",
       "      <th>анемия</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A1  A2  A3  A5  A9  A10  A11  A19  A20  A21  A23  A24  A25  A26  A28  A29  \\\n",
       "0   1   1   0   0   0    0    0    0    0    0    0    0    0    0    0    0   \n",
       "1   0   0   1   0   0    0    0    0    0    0    0    1    0    0    0    0   \n",
       "2   0   0   0   0   0    0    1    0    0    0    0    0    0    0    0    0   \n",
       "3   0   0   0   0   1    0    1    0    0    0    0    0    0    0    0    0   \n",
       "4   0   1   0   0   0    0    0    0    0    0    0    0    0    0    0    0   \n",
       "\n",
       "   A30  A31  A32  A33  A36  B5  B7  B8  B12  B13  B14  B15  B16  B17  B18  \\\n",
       "0    0    0    0    0    0   0   0   0    0    0    0    0    0    0    0   \n",
       "1    0    0    0    0    0   0   1   0    0    0    0    0    0    0    0   \n",
       "2    0    0    0    0    0   0   0   0    0    0    1    0    0    0    0   \n",
       "3    0    0    0    0    0   0   0   0    0    0    0    0    1    0    0   \n",
       "4    0    0    0    0    0   0   0   0    0    0    0    0    0    0    0   \n",
       "\n",
       "   B21  B22  B27  B34  B35  B37  B38  B39  B40  B41  B43  B44  B45  B47  B48  \\\n",
       "0    0    0    0    0    1    0    1    1    0    0    0    0    0    0    0   \n",
       "1    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0   \n",
       "2    0    0    0    0    1    0    0    0    0    0    0    0    0    0    0   \n",
       "3    0    0    0    0    1    0    0    0    0    0    0    0    0    0    0   \n",
       "4    0    0    0    0    0    0    0    0    1    0    0    0    0    0    0   \n",
       "\n",
       "   B49  B50  B51  B52  B55  B56  B57  B60  B61  B62  B70  B81  анемия  \n",
       "0    0    0    0    0    0    0    0    0    0    0    0    0       0  \n",
       "1    0    0    1    1    0    0    0    0    0    0    0    0       0  \n",
       "2    0    0    0    0    0    0    0    0    0    0    0    0       0  \n",
       "3    0    0    0    0    0    0    0    0    0    0    0    0       0  \n",
       "4    0    0    0    0    0    0    0    0    0    0    0    0       0  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/clean.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разбиение на валидационную и тренировочную выборки со стратификацией по искомому полю\n",
    "# Для показания результатов это надо:\n",
    "#   В валидационной и тренировочной выборке одинаковое соотношение \n",
    "#   классов \"с анемией\" и \"без анемии\"\n",
    "# Для финальной модели это плохо:\n",
    "#   Модель могла \"не увидеть\" некоторые примеры \"с анемией\"\n",
    "#   про примеры \"без анемии\" я молчу, их очень много\n",
    "df_train, df_test = train_test_split(df, test_size=0.20, stratify=df['анемия'], shuffle=True, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_train.pop('анемия')\n",
    "x_train = df_train.copy()\n",
    "\n",
    "y_test = df_test.pop('анемия')\n",
    "x_test = df_test.copy()\n",
    "\n",
    "tt = df.copy()\n",
    "y = tt.pop('анемия')\n",
    "x = tt.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(clf, name: str) -> list[float]:\n",
    "    clf.fit(x_train, y_train)\n",
    "    Y_pred = clf.predict(x_test)\n",
    "    clf.score(x_train, y_train)\n",
    "    train_set = round(clf.score(x_train, y_train) * 100, 2)\n",
    "    test_set = round(sum(Y_pred == y_test) / len(y_test) * 100, 2)\n",
    "    whole_set = round(clf.score(x, y) * 100, 2)\n",
    "\n",
    "    clear_output()\n",
    "    print('=========================================================')\n",
    "    print(f'Tested {name} classifier')\n",
    "    print(f'Success rate on train subset = {train_set}%')\n",
    "    print(f'Success rate on test subset = {test_set}%')\n",
    "    print(f'Success rate on whole dataset = {whole_set}%')\n",
    "    print('=========================================================')\n",
    "    return train_set, test_set, whole_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ура, сами модели  \n",
    "После отдельных блоков есть общая табличка, можете игнорировать и бежать сразу туда"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================\n",
      "Tested Random Forest classifier\n",
      "Success rate on train subset = 98.85%\n",
      "Success rate on test subset = 79.31%\n",
      "Success rate on whole dataset = 94.94%\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=1000, \n",
    "                             random_state=0)\n",
    "random_forest_res = test_model(rfc, 'Random Forest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================\n",
      "Tested Logistic Regression classifier\n",
      "Success rate on train subset = 83.91%\n",
      "Success rate on test subset = 83.91%\n",
      "Success rate on whole dataset = 83.91%\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg_res = test_model(logreg, 'Logistic Regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================\n",
      "Tested Support Vector Machines classifier\n",
      "Success rate on train subset = 83.91%\n",
      "Success rate on test subset = 83.91%\n",
      "Success rate on whole dataset = 83.91%\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "svc = SVC()\n",
    "svc_res = test_model(svc, 'Support Vector Machines')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. k-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================\n",
      "Tested k-Nearest Neighbors classifier\n",
      "Success rate on train subset = 87.07%\n",
      "Success rate on test subset = 80.46%\n",
      "Success rate on whole dataset = 85.75%\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn_res = test_model(knn, 'k-Nearest Neighbors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================\n",
      "Tested Gaussian Naive Bayes classifier\n",
      "Success rate on train subset = 30.75%\n",
      "Success rate on test subset = 33.33%\n",
      "Success rate on whole dataset = 31.26%\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "gaussian = GaussianNB()\n",
    "gaussian_res = test_model(gaussian, 'Gaussian Naive Bayes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================\n",
      "Tested Perceptron classifier\n",
      "Success rate on train subset = 83.33%\n",
      "Success rate on test subset = 80.46%\n",
      "Success rate on whole dataset = 82.76%\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "perceptron = Perceptron()\n",
    "perceptron_res = test_model(perceptron, 'Perceptron')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================\n",
      "Tested Linear SVC classifier\n",
      "Success rate on train subset = 84.48%\n",
      "Success rate on test subset = 83.91%\n",
      "Success rate on whole dataset = 84.37%\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "linear_svc = LinearSVC()\n",
    "linear_svc_res = test_model(linear_svc, 'Linear SVC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================\n",
      "Tested Stochastic Gradient Descent classifier\n",
      "Success rate on train subset = 71.26%\n",
      "Success rate on test subset = 63.22%\n",
      "Success rate on whole dataset = 69.66%\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "sgd = SGDClassifier()\n",
    "sgd_res = test_model(sgd, 'Stochastic Gradient Descent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Test Score</th>\n",
       "      <th>Train Score</th>\n",
       "      <th>Whole set Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Support Vector Machines</td>\n",
       "      <td>83.91</td>\n",
       "      <td>83.91</td>\n",
       "      <td>83.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>83.91</td>\n",
       "      <td>83.91</td>\n",
       "      <td>83.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>83.91</td>\n",
       "      <td>84.48</td>\n",
       "      <td>84.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KNN</td>\n",
       "      <td>80.46</td>\n",
       "      <td>87.07</td>\n",
       "      <td>85.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>80.46</td>\n",
       "      <td>83.33</td>\n",
       "      <td>82.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>79.31</td>\n",
       "      <td>98.85</td>\n",
       "      <td>94.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Stochastic Gradient Decent</td>\n",
       "      <td>63.22</td>\n",
       "      <td>71.26</td>\n",
       "      <td>69.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>33.33</td>\n",
       "      <td>30.75</td>\n",
       "      <td>31.26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Model  Test Score  Train Score  Whole set Score\n",
       "0     Support Vector Machines       83.91        83.91            83.91\n",
       "2         Logistic Regression       83.91        83.91            83.91\n",
       "7                  Linear SVC       83.91        84.48            84.37\n",
       "1                         KNN       80.46        87.07            85.75\n",
       "5                  Perceptron       80.46        83.33            82.76\n",
       "3               Random Forest       79.31        98.85            94.94\n",
       "6  Stochastic Gradient Decent       63.22        71.26            69.66\n",
       "4                 Naive Bayes       33.33        30.75            31.26"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = pd.DataFrame({\n",
    "    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression',\n",
    "              'Random Forest', 'Naive Bayes', 'Perceptron',\n",
    "              'Stochastic Gradient Decent', 'Linear SVC'],\n",
    "    'Test Score': [svc_res[1], knn_res[1], logreg_res[1],\n",
    "                   random_forest_res[1], gaussian_res[1], perceptron_res[1],\n",
    "                   sgd_res[1], linear_svc_res[1]],\n",
    "\n",
    "    'Train Score': [svc_res[0], knn_res[0], logreg_res[0],\n",
    "              random_forest_res[0], gaussian_res[0], perceptron_res[0],\n",
    "              sgd_res[0], linear_svc_res[0]],\n",
    "    'Whole set Score': [svc_res[2], knn_res[2], logreg_res[2],\n",
    "                        random_forest_res[2], gaussian_res[2], perceptron_res[2],\n",
    "                        sgd_res[2], linear_svc_res[2]]})\n",
    "models.sort_values(by=['Test Score'], ascending=False)\n",
    "\n",
    "# Пожалуйста, игнорируйте левые индексы и очередность разных оценок\n",
    "# Здесь идет сортировка по тестовой выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Train Score</th>\n",
       "      <th>Test Score</th>\n",
       "      <th>Whole set Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>98.85</td>\n",
       "      <td>79.31</td>\n",
       "      <td>94.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KNN</td>\n",
       "      <td>87.07</td>\n",
       "      <td>80.46</td>\n",
       "      <td>85.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>84.48</td>\n",
       "      <td>83.91</td>\n",
       "      <td>84.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Support Vector Machines</td>\n",
       "      <td>83.91</td>\n",
       "      <td>83.91</td>\n",
       "      <td>83.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>83.91</td>\n",
       "      <td>83.91</td>\n",
       "      <td>83.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>83.33</td>\n",
       "      <td>80.46</td>\n",
       "      <td>82.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Stochastic Gradient Decent</td>\n",
       "      <td>71.26</td>\n",
       "      <td>63.22</td>\n",
       "      <td>69.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>30.75</td>\n",
       "      <td>33.33</td>\n",
       "      <td>31.26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Model  Train Score  Test Score  Whole set Score\n",
       "3               Random Forest        98.85       79.31            94.94\n",
       "1                         KNN        87.07       80.46            85.75\n",
       "7                  Linear SVC        84.48       83.91            84.37\n",
       "0     Support Vector Machines        83.91       83.91            83.91\n",
       "2         Logistic Regression        83.91       83.91            83.91\n",
       "5                  Perceptron        83.33       80.46            82.76\n",
       "6  Stochastic Gradient Decent        71.26       63.22            69.66\n",
       "4                 Naive Bayes        30.75       33.33            31.26"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models.sort_values(by=['Whole set Score'], ascending=False)\n",
    "\n",
    "# А здесь сортировка по всему датасету"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На что стоит обратить внимание в табличке выше (первая сырая прогонка моделей классического ML):\n",
    "\n",
    "![Табличка с подчеркиваниями](images/table1.jpg)\n",
    "\n",
    "`Random forest` ПРЕКРАСНО себя показывает на целом датасете, но учитывая, что его `Test score` один из низших, а `Train score` наивысший - я боюсь оверфита... В принципе это и самая гибкая модель, возможно стоит поиграть с гиперпараметрами  \n",
    "\n",
    "`SVC`, `Linear SVC` и `LogReg` показывают себя одинаково хорошо на всех выборках, ставлю лайк  \n",
    "\n",
    "`Naive Bayes` и `SGD` идут спать  \n",
    "\n",
    "`Perceptron` и `KNN` скорее всего, хоть и результат у них неплохой  \n",
    "\n",
    "Дальше попробую поиграться с `Random forest` и DNN модельками"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Neaural Net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras import regularizers\n",
    "from tensorflow.keras.optimizers import Adamax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# перегон данных в формат, который понравится DNN\n",
    "tf_x_train = tf.convert_to_tensor(x_train)\n",
    "tf_y_train = tf.convert_to_tensor(y_train)\n",
    "tf_y_train = tf.one_hot(tf_y_train, 2)\n",
    "\n",
    "tf_x_test = tf.convert_to_tensor(x_test)\n",
    "tf_y_test = tf.convert_to_tensor(y_test)\n",
    "tf_y_test = tf.one_hot(tf_y_test, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сама модель, я пробовал много разных, делать модель шире и глубже смысла нет - результат одинаковый\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Input(shape = (58,)))\n",
    "model.add(Dense(32, kernel_regularizer=regularizers.l2(l=0.016), activity_regularizer=regularizers.l1(0.006),\n",
    "          bias_regularizer=regularizers.l1(0.006), activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dense(128, kernel_regularizer=regularizers.l2(l=0.016), activity_regularizer=regularizers.l1(0.006),\n",
    "#                 bias_regularizer=regularizers.l1(0.006), activation='relu'))\n",
    "# model.add(Dense(512, kernel_regularizer=regularizers.l2(l=0.016), activity_regularizer=regularizers.l1(0.006),\n",
    "#           bias_regularizer=regularizers.l1(0.006), activation='relu'))\n",
    "# model.add(Dense(128, kernel_regularizer=regularizers.l2(l=0.016), activity_regularizer=regularizers.l1(0.006),\n",
    "#           bias_regularizer=regularizers.l1(0.006), activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(Adamax(learning_rate = 0.01),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(label_smoothing = 0.1),\n",
    "              metrics = ['Accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "22/22 [==============================] - 1s 21ms/step - loss: 0.8795 - Accuracy: 0.8247 - val_loss: 0.6454 - val_Accuracy: 0.8391\n",
      "Epoch 2/10\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.5988 - Accuracy: 0.8391 - val_loss: 0.5686 - val_Accuracy: 0.8391\n",
      "Epoch 3/10\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.5548 - Accuracy: 0.8391 - val_loss: 0.5492 - val_Accuracy: 0.8391\n",
      "Epoch 4/10\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.5357 - Accuracy: 0.8391 - val_loss: 0.5375 - val_Accuracy: 0.8391\n",
      "Epoch 5/10\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.5280 - Accuracy: 0.8391 - val_loss: 0.5342 - val_Accuracy: 0.8391\n",
      "Epoch 6/10\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.5228 - Accuracy: 0.8391 - val_loss: 0.5274 - val_Accuracy: 0.8391\n",
      "Epoch 7/10\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.5157 - Accuracy: 0.8391 - val_loss: 0.5245 - val_Accuracy: 0.8391\n",
      "Epoch 8/10\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.5128 - Accuracy: 0.8391 - val_loss: 0.5225 - val_Accuracy: 0.8391\n",
      "Epoch 9/10\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.5085 - Accuracy: 0.8391 - val_loss: 0.5216 - val_Accuracy: 0.8391\n",
      "Epoch 10/10\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.5072 - Accuracy: 0.8391 - val_loss: 0.5205 - val_Accuracy: 0.8391\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f09c6ba320>"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(tf_x_train, tf_y_train,\n",
    "    epochs = 10,\n",
    "    batch_size = 16,\n",
    "    validation_data = (tf_x_test, tf_y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 3ms/step\n",
      "Accuracy: 80.46%\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(tf_x_test)\n",
    "res = []\n",
    "for idx in range(len(preds)):\n",
    "    res.append(np.argmax(y[idx]) == np.argmax(preds[idx]))\n",
    "    # print(f'True idx = {np.argmax(true[idx])}')\n",
    "    # print(f'Predicted idx = {np.argmax(preds[idx])}')\n",
    "print(f'Accuracy: {np.sum(y) / np.sum(res)*100:.04}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mat = confusion_matrix(y_test, res)\n",
    "\n",
    "sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False, cmap='Blues',\n",
    "            xticklabels=['Positive', 'Negative'], yticklabels=['True', 'False'])\n",
    "\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('Actual label')\n",
    "plt.show()\n",
    "# Вообще вот это очень важный кусок информации, дальше конкретнее"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "----------------------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False     0.8391    1.0000    0.9125        73\n",
      "        True     0.0000    0.0000    0.0000        14\n",
      "\n",
      "   micro avg     0.8391    0.8391    0.8391        87\n",
      "   macro avg     0.4195    0.5000    0.4562        87\n",
      "weighted avg     0.7041    0.8391    0.7657        87\n",
      " samples avg     0.8391    0.8391    0.8391        87\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Тоже самое, что и confusion matrix, но немного в другом формате\n",
    "preds = (preds > 0.5)\n",
    "clr = classification_report(tf_y_test, preds, \\\n",
    "                            target_names=['False', 'True'], \\\n",
    "                            digits=4, \\\n",
    "                            zero_division = 0)\n",
    "print(f\"Classification Report:\\n----------------------\\n\", clr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Небольшие выводы\n",
    "\n",
    "Нейронная сеть показывает **неплохой результат**. Более важную информацию мне дает **матрица несоответсвий (confusion matrix)**:\n",
    "\n",
    "`100% recall` радует - все, кого модель посчитала `\"положительными\"` *(то есть те, кто могут пострадать от анемии)* - действительно перенесут/перенесли анемию.  \n",
    "С другой стороны, `неполнота precision` настораживает - примрено `16%` людей, *кому алгоритм предскажет анемию* от нее на самом то деле не пострадают.  \n",
    "Во всех подобных алгоритмах стоит **балансировать** между precision и recall и ***я пока не знаю*** что я стараться максимизировать: нахождение реальны подозрений на заболевание или отсутствие ложных срабатывайний...  \n",
    "\n",
    "Если честно, ***я удивлен такими результатами***. Я ожидал что recall будет ниже, т.к. количество людей с анемией в датасете в *5 раз меньше*. С другой стороны *я мог просто очень крупно обосраться с интепритацией*, и на самом деле все цифры абсолютно наоборот и precision = recall, recall = precision.   \n",
    "\n",
    "Учитывая схожесть результатов нескольких классических ML моделей и нейронной сети (DNN), толькой для которой я провел более глубокую валидацию *(пусть от лени проводить на остальных моделях)* и считаю, что скорее всего ***результаты по F1, recall и precision у них схожи***; назревает вывод - для более точных предсказаний стоит: \n",
    "1. балансировать и чистить данные глубже (отмечал в другом ноутбуке); \n",
    "2. пробовать вводить дополнительные параметры (возраст, пол, предрасположенность (если это играет роль));  \n",
    "\n",
    "Ну и ответ про точность лучших моделей: `83.91%` в худшем случае (по precision), `91.25%` в среднем (по F1).\n",
    "Реальную \"важностную\" общую точность (смотря что нам более выжно, см. выше) можно высчитать, выдав precision и recall разные веса."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "20f5b49acdb8aa5482bf611cd625371fff780f4a1ea28b5df1442e16c229ca99"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
